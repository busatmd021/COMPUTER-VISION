{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVr6zh_RnvXC"
      },
      "source": [
        "# **Computer Vision 2025 Assignment 1:** Image Filtering\n",
        "In this assignment, you will research, implement and test some image filtering operations. Image filtering by convolution is a fundamental step in many computer vision tasks and you will find it useful to have a firm grasp of how it works. For example, later in the course we will come across Convolutional Neural Networks (CNNs) which are built from convolutional image filters.\n",
        "\n",
        "**The main aims of the assignment are:**\n",
        "- To understand the basics of how images are stored and processed in memory;\n",
        "- To gain exposure to several common image filters, and understand how they work;\n",
        "- To get practical experience implementing convolutional image filters;\n",
        "- To test your intuition about image filtering by running some experiments;\n",
        "- To report your results in a clear and concise manner.\n",
        "\n",
        "***This assignment relates to the following ACS CBOK areas:*** *abstraction, design, hardware and software, data and information, HCI and programming.*\n",
        "<br/><br/><br/>\n",
        "\n",
        "\n",
        "## **General instructions**\n",
        "Follow the instructions in this Python notebook and the accompanying file *a1code.py* to answer each question. It's your responsibility to make sure your answer to each question is clearly labelled and easy to understand. Note that most questions require some combination of Python code, graphical output, and text analysing or describing your results. Although we will check your code as needed, marks will be assigned based on the quality of your write up rather than for code correctness! This is not a programming test - we are more interested in your understanding of the topic.\n",
        "\n",
        "Only a small amount of code is required to answer each question. We will make extensive use of the Python libraries\n",
        "- [numpy](numpy.org) for mathematical functions\n",
        "- [skimage](https://scikit-image.org) for image loading and processing\n",
        "- [matplotlib](https://matplotlib.org/stable/index.html) for displaying graphical results\n",
        "- [jupyter](https://jupyter.org) for Jupyter Notebooks\n",
        "\n",
        "You should get familiar with the documentation for these libraries so that you can use them effectively.<br/><br/><br/>\n",
        "\n",
        "# **The Questions**\n",
        "To get started, below is some setup code to import the libraries we need. You should not need to edit it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTzS8va6nvXI"
      },
      "outputs": [],
      "source": [
        "# Numpy is the Main Package for Ccientific Computing with Python.\n",
        "import numpy as np\n",
        "\n",
        "# Imports all the Methods we Define in the File a1code.py\n",
        "from a1code import *\n",
        "\n",
        "# Matplotlib is a Useful Plotting Library for Python\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# This code is to Make Matplotlib Figures Appear Inline in the Notebook Rather than in a New Window.\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0)    # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# Some More Magic so that the Notebook will Reload External Python Modules;\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%reload_ext autoreload"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ye5axCbnvXJ"
      },
      "source": [
        "<br/><br/>\n",
        "\n",
        "## **Question 0: Numpy Warm Up! *(Not Assesed)***\n",
        "\n",
        "Before starting the assignment, make sure you have a working Python 3 installation, with up to date versions of the libraries mentioned above. If this is all new to you, I'd suggest  downloading an all in one Python installation such as [Anaconda](https://www.anaconda.com/products/individual). Alternatively you can use a Python package manager such as pip or conda, to get the libraries you need. If you're struggling with this please ask a question on the MyUni discussion forum.\n",
        "\n",
        "For this assignment, you need some familiarity with numpy syntax. The numpy QuickStart should be enough to get you started:\n",
        "\n",
        "https://numpy.org/doc/stable/user/quickstart.html\n",
        "\n",
        "Here are a few warm up exercises to make sure you understand the basics. Answer them in the space below. Be sure to print the output of each question so we can see it!\n",
        "\n",
        "1. Create a 1D numpy array Z with 12 elements. Fill with values 1 to 12.\n",
        "2. Reshape Z into a 2D numpy array A with 3 rows and 4 columns.\n",
        "3. Reshape Z into a 2D numpy array B with 4 rows and 3 columns.\n",
        "4. Calculate the *matrix* product of A and B.\n",
        "5. Calculate the *element wise* product of $A$ and $B^T$ (B transpose).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcCk1Jt6nvXK"
      },
      "outputs": [],
      "source": [
        "# Create a 1D Numpy Array Z with 12 Elements. Fill with Values 1 to 12.\n",
        "Z1 = np.array([1,2,3,4,5,6,7,8,9,10,11,12])\n",
        "Z2 = np.arange(1,13)\n",
        "print(Z1)\n",
        "print(Z2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reshape Z into a 2D Numpy Array A with 3 Rows & 4 Columns.\n",
        "A = Z1.reshape(3,4)\n",
        "print(A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reshape Z into a 2D Numpy Array B with 4 Rows & 3 Columns.\n",
        "B = Z2.reshape(4,3)\n",
        "print(B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the Matrix Product of A & B.\n",
        "product = np.dot(A,B)\n",
        "print(product)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the Element Wise product of A & B^T (B transpose)\n",
        "transpose = B.transpose()\n",
        "element = A * transpose\n",
        "print(element)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B60zKcbpnvXK"
      },
      "source": [
        "***You need to be comfortable with numpy arrays because that is how we store images. Let's do that next!***<br/><br/><br/>\n",
        "\n",
        "## **Question 1: Loading & Displaying an Image *(10%)***\n",
        "Below is a function to display an image using the pyplot module in matplotlib. Implement the `load()` and `print_stats()` functions in a1code.py so that the following code loads the whipbird image, displays it and prints its height, width and channel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9wLpQz-nvXL"
      },
      "outputs": [],
      "source": [
        "# Format the Display of Images in the Notebook\n",
        "def display(img, caption=''):\n",
        "    # Show Image using Pyplot\n",
        "    plt.figure()\n",
        "    plt.imshow(img)\n",
        "    plt.title(caption)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4Rp8XW7nvXM"
      },
      "outputs": [],
      "source": [
        "# Load the Image\n",
        "image0 = load('images/whipbird.jpg')\n",
        "\n",
        "# Display the Image Below\n",
        "display(image0, 'whipbird')\n",
        "\n",
        "# Print the Height, Width & No. Channels \n",
        "print_stats(image0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCONJ-XunvXN"
      },
      "source": [
        "Return to this question after reading through the rest of the assignment. Find **at least 2 more images** to use as test cases in this assignment for all the following questions and display them below. Use your print_stats() function to display their height, width and number of channels. Explain *why* you have chosen each image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4-kgWamnvXN"
      },
      "outputs": [],
      "source": [
        "# Display Wooden Texture Image\n",
        "image0 = load('images/wooden.png')\n",
        "display(image0, 'wooden')\n",
        "print_stats(image0)\n",
        "\n",
        "# Display Portrait Image\n",
        "image2 = load('images/portrait.png')\n",
        "display(image2, 'Portrait')\n",
        "print_stats(image2)\n",
        "\n",
        "# Display Tiger (Speckle/Noisy) Image\n",
        "image3 = load('images/speckle.jpg')\n",
        "image3_grey = image3[..., 0]  # Take just the first channel\n",
        "display(image3_grey, 'Tiger')\n",
        "print_stats(image3_grey)\n",
        "\n",
        "# Display Tree Image\n",
        "image4 = load('images/tree.png')\n",
        "image4_grey = image4[..., 0]  # Take just the first channel\n",
        "display(image4_grey, 'Greyscale Tree')\n",
        "print_stats(image4_grey)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teBs9aY_nvXO"
      },
      "source": [
        "**Wooden Fence Texture**\n",
        "The wooden fence texture is selected for testing sharpening filters, such as the Laplacian or Unsharp Mask, to enhance fine details. This texture provides a clear structure that will help in evaluating the effectiveness of these filters. Additionally, it can be used to explore texture-preserving smoothing methods like bilateral filtering, allowing for a better understanding of how these filters preserve important features of the texture while reducing noise or smoothing the background.\n",
        "\n",
        "**Portrait Image *(Person's Face)***\n",
        "The portrait image is chosen to explore skin smoothing techniques such as bilateral filtering or Gaussian blur. Faces require careful treatment in image processing, where you want to smooth the skin without losing sharp details around facial features. This image will allow for testing edge-preserving filters, ensuring that facial features like eyes, lips, and nose are kept sharp while the skin is smoothed for a natural look.\n",
        "\n",
        "**Noisy Tiger Image *(Salt-and-Pepper Noise)***\n",
        "This image is chosen to test denoising filters such as median filtering and Gaussian smoothing. The presence of salt-and-pepper noise makes it a perfect candidate for assessing how well these filters can remove noise while preserving edges. It also offers a useful scenario for testing edge detection techniques after denoising, providing insights into how well different methods handle noise without losing significant image details.\n",
        "\n",
        "**Grayscale Tree Image**\n",
        "This grayscale image of a tree is chosen to test image processing techniques that focus on handling single-channel images. Since the task requires working with grayscale images, this image will help assess how well the applied filters, such as denoising, edge detection, and contrast enhancement, perform on a simple but structured image. The tree's natural details and varying intensities across the branches and leaves make it an ideal candidate for evaluating how well these techniques preserve or enhance key features in a grayscale image.<br/><br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Task 1.1**\n",
        "Now that you can read an image from file and display it, apply some simple point processes to your images, such as those described in Lecture 1, and observe the results. **What happens when a processed pixel value becomes < 0 or > 255? What effect does this have on later processing?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementation of an Inversion Point Process \n",
        "image4_grey_inverse = 1 - image4_grey \n",
        "display(image4_grey_inverse, 'Inverted Tree')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementation of Darken\n",
        "image4_grey_dark = (image4_grey - 0.5)\n",
        "display(image4_grey_dark, 'Darkened Tree')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementation of Non-Linear Lower Contrast\n",
        "image4_grey_nonLin = (((image4_grey/1) ** (1/2)) * 1)\n",
        "display(image4_grey_nonLin, 'Non-Linear Lower Contrast Tree')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When a processed pixel value becomes less than 0 or greater than 1 *(since the above images are floating points **i.e** divided by 255)*, it can lead to unintended visual and computational effects. In the darkening transformation `(image4_grey - 0.5)`, some pixel values fall below zero. Since the `matplotlib` library used in the `display` function does not automatically clip negative values for floating-point images, these negative values will be displayed as extreme black pixels, while values greater than 1 will appear as pure white. This behavior is due to how `matplotlib` handles the display of out-of-range values.\n",
        "\n",
        "These out-of-range values can impact later processing in different ways. Negative pixel values may not always be clipped but could be misinterpreted by functions expecting only non-negative inputs, leading to unpredictable results. If pixel values exceed the expected range, functions assuming normalisation *(**e.g.,** filters or edge detection models)* may behave unexpectedly, potentially distorting contrast or brightness adjustments. Ensuring that operations maintain the [0,1] range for floating point images prevents these issues and ensures that subsequent image processing steps work as intended.\n",
        "<br/><br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Task 1.2**\n",
        "Now repeat the above for a RGB test image, but this time, apply the point processes only to one channel of the image *(red, green or blue)*. Display the resulting RGB image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementation of an Inversion Point Process on the Blue Channel\n",
        "# Isolate the Blue Channel & Invert it\n",
        "image2_blue = image2[..., 2]\n",
        "image2_inverse = 1 - image2_blue \n",
        "\n",
        "# Replace the Inverted Blue Channel Back into the Image\n",
        "image2_modified = image2.copy()\n",
        "image2_modified[..., 2] = image2_inverse\n",
        "\n",
        "# Display the Image\n",
        "display(image2_modified, 'Person with Inverted Blue Channel')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The blue channel of the image is isolated and inverted by flipping the pixel values within the [0, 1] range. This process causes lighter pixels in the blue channel to become darker, and darker pixels to become lighter, demonstrating how manipulating individual colour channels can create significant changes in the overall image appearance.\n",
        "<br/><br/><br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0C4vaWAnvXP",
        "tags": []
      },
      "source": [
        "## **Question 2: Image processing *(30%)***\n",
        "\n",
        "Now that you have an image stored as a numpy array, let's try some operations on it.\n",
        "\n",
        "1. Implement the `crop()` function in a1code.py. Use array slicing to crop the image.\n",
        "2. Implement the `resize()` function in a1code.py.\n",
        "3. Implement the `change_contrast()` function in a1code.py.\n",
        "4. Implement the `greyscale()` function in a1code.py.\n",
        "5. Implement the `binary()` function in a1code.py.\n",
        "\n",
        "**What do you observe when you change the threshold of the binary function?**\n",
        "\n",
        "**Apply all these functions with different parameters on your own test images and discuss the results.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wa-uPtH7nvXP"
      },
      "outputs": [],
      "source": [
        "# Testing the Bird Image on All Five Tests\n",
        "crop_img = crop(image0, 80, 180, 400, 272)\n",
        "display(crop_img)\n",
        "print_stats(crop_img)\n",
        "\n",
        "resize_img = resize(crop_img, 500, 600)\n",
        "display(resize_img)\n",
        "print_stats(resize_img)\n",
        "\n",
        "contrast_img = change_contrast(image0, 0.5)\n",
        "display(contrast_img)\n",
        "print_stats(contrast_img)\n",
        "\n",
        "contrast_img = change_contrast(image0, 1.5)\n",
        "display(contrast_img) \n",
        "print_stats(contrast_img)\n",
        "\n",
        "grey_img = greyscale(image0)\n",
        "display(grey_img)\n",
        "print_stats(grey_img)\n",
        "\n",
        "binary_img = binary(grey_img, 0.3)\n",
        "display(binary_img)\n",
        "print_stats(binary_img)\n",
        "\n",
        "binary_img = binary(grey_img, 0.7)\n",
        "display(binary_img)\n",
        "print_stats(binary_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing on the Person Image\n",
        "crop_img = crop(image2, 40, 130, 400, 272)\n",
        "display(crop_img)\n",
        "print_stats(crop_img)\n",
        "\n",
        "resize_img = resize(crop_img, 200, 140)\n",
        "display(resize_img)\n",
        "print_stats(resize_img)\n",
        "\n",
        "contrast_img = change_contrast(image2, 0.3)\n",
        "display(contrast_img)\n",
        "print_stats(contrast_img)\n",
        "\n",
        "contrast_img = change_contrast(image2, 1.9)\n",
        "display(contrast_img) \n",
        "print_stats(contrast_img)\n",
        "\n",
        "grey_img = greyscale(image2)\n",
        "display(grey_img)\n",
        "print_stats(grey_img)\n",
        "\n",
        "binary_img = binary(grey_img, 0.2)\n",
        "display(binary_img)\n",
        "print_stats(binary_img)\n",
        "\n",
        "binary_img = binary(grey_img, 0.83)\n",
        "display(binary_img)\n",
        "print_stats(binary_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing on the Person Image\n",
        "crop_img = crop(image4, 260, 615, 950, 1400)\n",
        "display(crop_img)\n",
        "print_stats(crop_img)\n",
        "\n",
        "resize_img = resize(crop_img, 350, 500)\n",
        "display(resize_img)\n",
        "print_stats(resize_img)\n",
        "\n",
        "contrast_img = change_contrast(image4, 0.6)\n",
        "display(contrast_img)\n",
        "print_stats(contrast_img)\n",
        "\n",
        "contrast_img = change_contrast(image4, 1.25)\n",
        "display(contrast_img) \n",
        "print_stats(contrast_img)\n",
        "\n",
        "grey_img = greyscale(image4)\n",
        "display(grey_img)\n",
        "print_stats(grey_img)\n",
        "\n",
        "binary_img = binary(grey_img, 0.15)\n",
        "display(binary_img)\n",
        "print_stats(binary_img)\n",
        "\n",
        "binary_img = binary(grey_img, 0.75)\n",
        "display(binary_img)\n",
        "print_stats(binary_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Cropping Function:**\n",
        "Cropping the image worked well and produced good results when the cropping window was correctly positioned. However, careful adjustment was required to ensure the selected region contained the intended details. If the crop coordinates were off even slightly, important parts of the image were lost. In the use cases above, it is only after the cropping area is carefully positioned that the function effectively extracts the region of interest without distortion.\n",
        "\n",
        "**Resizing Function:**\n",
        "Resizing had a noticeable impact on image quality. As seen in the reduction image tests, details were lost because fewer pixels were available to represent the same visual information. This was especially visible in the portrait image, where fine textures became less clear as the image size decreased. The effect was particularly bad when downsizing significantly, as pixel selection caused the image to appear blocky and less detailed since the new neighbouring pixels were orignally further apart from one another. \n",
        "\n",
        "**Contrast Adjustment:**\n",
        "Applying contrast adjustments produced good results overall. The enhancement made differences between light and dark areas more pronounced, improving clarity without significantly degrading image quality. Both low and high-contrast settings worked well in different ways. In the portrait image, contrast adjustments made facial features clearer and more defined, but extreme settings could still reduce depth slightly. The tree image responded particularly well, as the increased contrast helped highlight the branches and shadows, making the structure of the tree more visible and well-defined. In both cases, contrast adjustment was an effective tool for enhancing key details without introducing significant artifacts.\n",
        "\n",
        "**Greyscale Conversion:**\n",
        "Converting images to greyscale worked as expected, with all the output images having only one channel, confirming that no unexpected duplication occurred. Additionally, the transformation effectively preserved essential details across different images. In the tree image, the shadows and branches remained well-defined, maintaining the depth and texture of the original scene. The portrait image also retained clear distinctions between facial features and the background, ensuring that important details were not lost despite the removal of colour information.\n",
        "\n",
        "**Binary Thresholding:**\n",
        "Binary thresholding produced significant variations based on the threshold level.\n",
        "- **Lower threshold →** More white pixels appeared, emphasising bright areas. This was particularly effective for the portrait image which was a naturally birhgter image. Here a lower threshold captured a clean outline of the person’s edges against the background.\n",
        "- **Higher threshold →** More black pixels appeared, preserving darker features. In the tree image, increasing the threshold actually maintained more of the tree’s structure because the image already contained a lot of shadows.\n",
        "\n",
        "This showed that the optimal threshold depends on the existing light and shadow distribution in the image. The binary function was reasonably effective for separating objects from their background, but choosing the right threshold was critical for getting meaningful results.\n",
        "<br/><br/><br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx6w6z0ynvXP"
      },
      "source": [
        "## **Question 3: Convolution *(30%)***\n",
        "\n",
        "### **Task 3.1(a):** 2D Convolution\n",
        "\n",
        "Using the definition of 2D convolution from week 1, implement the convolution operation in the function `conv2D()` in a1code.py."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIkQld3envXQ"
      },
      "outputs": [],
      "source": [
        "test_conv2D()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxsYnQQPnvXQ"
      },
      "source": [
        "<br/><br/>\n",
        "\n",
        "### **Task 3.1(b):** RGB Convolution\n",
        "\n",
        "In the function `conv` in a1code.py, extend your function `conv2D` to work on RGB images, by applying the 2D convolution to each channel independently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Code?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv82cYbgnvXQ"
      },
      "source": [
        "<br/><br/>\n",
        "\n",
        "### **Task 3.2:** Gaussian Filter Convolution\n",
        "\n",
        "Use the `gauss2D` function provided in a1code.py to create a Gaussian kernel, and apply it to your images with convolution. You will obtain marks for trying different tests and analysing the results, for example:\n",
        "\n",
        "- Try varying the image size, and the size and variance of the filter  \n",
        "- Subtract the filtered image from the original - this gives you an idea of what information is lost when filtering\n",
        "\n",
        "**What do you observe and why?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test code?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Answer to Question 3.2\n",
        "<br/><br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpnxewpanvXR"
      },
      "source": [
        "### **Task 3.3:** Sobel Filters\n",
        "\n",
        "Define a horizontal and vertical Sobel edge filter kernel and test them on your images. You will obtain marks for testing them and displaying results in interesting ways, for example:\n",
        "\n",
        "- Apply them to an image at different scales.\n",
        "- Consider how to display positive and negative gradients.\n",
        "- Apply different combinations of horizontal and vertical filters, such as applying the vertical Sobel filter to the output of the horizontal Sobel filter. \n",
        "\n",
        "**What do you see?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfLN6SvinvXR"
      },
      "outputs": [],
      "source": [
        "# Your code to answer 3.3, 3.4 and displaay results here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp9L6sfynvXR"
      },
      "source": [
        "#### Answer 3.3\n",
        "<br/><br/><br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCSysY1VnvXR",
        "tags": []
      },
      "source": [
        "## Question 4: Image sampling and pyramids (30%)\n",
        "\n",
        "### 4.1 Image Sampling\n",
        "\n",
        "- Apply your `resize()` function to reduce an image (I) to 0.5\\*height and 0.5\\*width\n",
        "\n",
        "- Repeat the above procedure, but apply a Gaussian blur filter to your original image before downsampling it. How does the result compare to your previous output, and to the original image? Why?\n",
        "\n",
        "\n",
        "### 4.2 Image Pyramids\n",
        "- Create a Gaussian pyramid as described in week2's lecture on an image.\n",
        "\n",
        "- Apply a Gaussian kernel to an image I, and resize it with ratio 0.5, to get $I_1$. Repeat this step to get $I_2$, $I_3$ and $I_4$.\n",
        "\n",
        "- Display these four images in a manner analogus to the example shown in the lectures.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tJYuApFnvXS"
      },
      "outputs": [],
      "source": [
        "# Your answers to question 4 here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23grHWRLnvXS"
      },
      "source": [
        "***Your comments/analysis of your results here...***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiwL3SpHnvXS"
      },
      "source": [
        "## Question 5: (optional, assesed for granting up to 20% bonus marks for the A1)\n",
        "\n",
        "Image filtering lectures, particularly Lecture 2, have covered the details related to this question. This is a bonus question for the students to get opportunities to recover lost marks in the other parts of the assignment. **Note that the overall marks will be capped at 100%**.\n",
        "\n",
        "### 5.1 Apply and analyse a blob detector\n",
        "\n",
        "- Create a Laplacian of Gaussian (LoG) filter in the function `LoG2D()` and visualise its response on your images. You can use the template function (and hints therein) for the task if you wish.\n",
        "\n",
        "- Modify parameters of the LoG filters and apply them to an image of your choice. Show how these variations are manifested in the output.\n",
        "\n",
        "- Repeat the experiment by rescaling the image with a combination of appropriate filters designed by you for these assignment. What correlations do you find when changing the scale or modifying the filters?\n",
        "\n",
        "- How does the response of LoG filter change when you rotate the image by 90 degrees? You can write a function to rotate the image or use an externally rotated image for this task.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_fvxboznvXS"
      },
      "outputs": [],
      "source": [
        "# Your code to answer question 5 and display results here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
